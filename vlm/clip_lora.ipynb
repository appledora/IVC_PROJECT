{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedMNIST2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MedMNIST data\n",
    "# !pip install medmnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from utils import load_data_and_data_loaders\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'octmnist'\n",
    "download = True\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "info = INFO[data_flag]\n",
    "label_dict = info[\"label\"]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "if data_flag == \"pathmnist\":\n",
    "    label_choices = [f\"a {label} tissue sample\" for label in label_dict.values()]\n",
    "elif data_flag == \"chestmnist\":\n",
    "    label_choices = [f\"an X-Ray image with {label} disease\" for label in label_dict.values()]\n",
    "elif data_flag == \"octmnist\":\n",
    "    label_choices = [f\"a Retinal OCT image with {label} disease\" for label in label_dict.values()]\n",
    "else:\n",
    "    label_choices = list(label_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "training_data, validation_data, test_data, training_loader, validation_loader, test_loader, _ = load_data_and_data_loaders(\"BLOCK\",\n",
    "                                                                                                                           data_flag,\n",
    "                                                                                                                           BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLIP - Low Rank Adaptation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torchmetrics.functional.classification import multiclass_accuracy, multiclass_auroc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - LoRA adapter\n",
    "\n",
    "Vision and Text Encoder\n",
    "* Q-projection head\n",
    "* V-projection head\n",
    "* Fully-connected 1 (Projection head) \n",
    "* Fully-connected 2 (Projection head) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_tune(model):\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    ## Loss function\n",
    "    loss_img = torch.nn.CrossEntropyLoss()\n",
    "    loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ## Fine-tuning layers\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=1e-5, weight_decay=0.00001)\n",
    "\n",
    "    num_batches_train = len(training_loader.dataset)/BATCH_SIZE\n",
    "\n",
    "    ## Training\n",
    "    for epoch in range(10):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        epoch_train_loss = 0\n",
    "        model.train()\n",
    "        for batch in tqdm(training_loader, total=num_batches_train):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ## Format input data\n",
    "            (x, labels) = batch\n",
    "            x = x.to(device)\n",
    "            text_labels = [f\"{label_dict[str(label.cpu().numpy()[0])]}\" for label in labels]\n",
    "            try:\n",
    "                inputs = processor(text=text_labels, images=x, return_tensors=\"pt\",\n",
    "                                do_rescale=False,\n",
    "                                do_center_crop=False,\n",
    "                                padding=True)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(device) # set each processed data to device\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "                logits_per_text = outputs.logits_per_text # this is the image-text similarity score\n",
    "\n",
    "                ground_truth = torch.arange(logits_per_image.shape[0], dtype=torch.long, device=device)\n",
    "\n",
    "                total_train_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "                total_train_loss.backward()\n",
    "                epoch_train_loss += total_train_loss\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(params, 5.0)\n",
    "\n",
    "                if device == \"cpu\":\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "            except Exception as e:\n",
    "                print(\"Unable to train due to: \", e)\n",
    "\n",
    "        epoch_train_loss /= num_batches_train\n",
    "\n",
    "        # Compute validation performance\n",
    "        model.eval()\n",
    "        acc_top1_list = []\n",
    "        acc_top3_list = []\n",
    "\n",
    "        num_batches_val = len(validation_loader.dataset)/BATCH_SIZE\n",
    "\n",
    "        for _, batch in enumerate(tqdm(validation_loader, total=num_batches_val)):\n",
    "            (val_x, val_labels) = batch\n",
    "            val_x = val_x.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            text_labels = [f\"a {label} tissue sample\" for label in label_dict.values()]\n",
    "\n",
    "            inputs = processor(text=text_labels, images=val_x, return_tensors=\"pt\",\n",
    "                                do_rescale=False,\n",
    "                                do_center_crop=False,\n",
    "                                padding=True)\n",
    "                                \n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device) # set each processed data to device\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            image_features, text_features = outputs.image_embeds, outputs.text_embeds\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "            acc_top1 = multiclass_accuracy(similarity, val_labels.squeeze(), num_classes=len(label_dict))\n",
    "            acc_top3 = multiclass_accuracy(similarity, val_labels.squeeze(), top_k=3, num_classes=len(label_dict))\n",
    "            acc_top1_list.append(acc_top1)\n",
    "            acc_top3_list.append(acc_top3)\n",
    "\n",
    "        print(f\"Epoch {epoch} train loss: {epoch_train_loss / num_batches_train}\")\n",
    "\n",
    "        # compute mean top3 accuracy and top1 accuracy\n",
    "        mean_top3_accuracy = torch.stack(acc_top3_list).mean().cpu().numpy()\n",
    "        print(f\"Mean Top 3 Accuracy: {mean_top3_accuracy*100}%.\")\n",
    "        mean_top1_accuracy = torch.stack(acc_top1_list).mean().cpu().numpy()\n",
    "        print(f\"Mean Top 1 Accuracy: {mean_top1_accuracy*100}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_tune(model=lora_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_path = Path(f\"./lora_model/{data_flag}\")\n",
    "os.makedirs(weights_path, exist_ok=True)\n",
    "torch.save({\"model_state_dict\": lora_model}, weights_path / f\"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLIP - Attention to Detail\n",
    "\n",
    "* #TODO: Attention visualization maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "lora_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clip(model, data_loader, save_dir):\n",
    "    auc_list = []\n",
    "    acc_top1_list = []\n",
    "    acc_top3_list = []\n",
    "    similarities = []\n",
    "    for _, batch in enumerate(tqdm(data_loader)):\n",
    "        (val_x, val_labels) = batch\n",
    "        val_x = val_x.to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "        text_labels = label_choices\n",
    "\n",
    "        inputs = processor(text=text_labels, images=val_x, return_tensors=\"pt\",\n",
    "                            do_rescale=False,\n",
    "                            do_center_crop=False,\n",
    "                            padding=True)\n",
    "                            \n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device) # set each processed data to device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        image_features, text_features = outputs.image_embeds, outputs.text_embeds\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "        acc_top1 = multiclass_accuracy(similarity, val_labels.squeeze(), num_classes=len(label_choices))\n",
    "        acc_top3 = multiclass_accuracy(similarity, val_labels.squeeze(), top_k=3, num_classes=len(label_choices))\n",
    "        auc = multiclass_auroc(similarity, val_labels.squeeze(), num_classes=len(label_choices))\n",
    "        acc_top1_list.append(acc_top1)\n",
    "        acc_top3_list.append(acc_top3)\n",
    "        auc_list.append(auc)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # compute mean top3 accuracy and top1 accuracy\n",
    "    mean_top3_accuracy = torch.stack(acc_top3_list).mean().cpu().numpy()\n",
    "    print(f\"Mean Top 3 Accuracy: {mean_top3_accuracy*100}%.\")\n",
    "    mean_top1_accuracy = torch.stack(acc_top1_list).mean().cpu().numpy()\n",
    "    print(f\"Mean Top 1 Accuracy: {mean_top1_accuracy*100}%.\")\n",
    "    mean_auc = torch.stack(auc_list).mean().cpu().numpy()\n",
    "    print(f\"Mean Current AUC: {mean_auc*100}%\")\n",
    "\n",
    "    predicted_similarities = torch.cat(similarities).cpu().numpy()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.save(f\"{save_dir}/val_preds.npy\", predicted_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_clip(model=lora_model, data_loader=test_loader, save_dir=f\"./lora_model/{data_flag}/preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Zero-shot CLIP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLIP - Zero-shot classification\n",
    "\n",
    "* Prompt:\n",
    "    * pathmnist: \"a {label} tissue sample\"\n",
    "    * octmnist:  \"a Retinal OCT image with {label} disease\"\n",
    "\n",
    "* class_label := INFO[data_flag][\"label\"].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions saved through zero shot classification using CLIP\n",
    "# ref: clip_zero_shot_classification.py\n",
    "\n",
    "train_preds = np.load(f\"./results/zero-shot/train_{data_flag}.npy\")\n",
    "val_preds = np.load(f\"./results/zero-shot/val_{data_flag}.npy\")\n",
    "test_preds = np.load(f\"./results/zero-shot/test_{data_flag}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist.evaluator import getACC, getAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(true, preds, info, split: str=\"Train\"):\n",
    "    # Accuracy\n",
    "    topk_acc = getACC(true, preds, task=info[\"task\"])\n",
    "    print(f\"Accuracy: {topk_acc}\")\n",
    "    \n",
    "    # AUC\n",
    "    micro_roc_auc_ovr = getAUC(true, preds, task=info[\"task\"])\n",
    "\n",
    "    print(f\"ROC AUC score: {micro_roc_auc_ovr:.4f}\")\n",
    "\n",
    "evaluation(test_data.labels.squeeze(), test_preds, info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
